{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import random\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é¢„è¯»å–ä¸é¢„å¤„ç† _ç”¨äºLDAæ¨¡å‹\n",
    "def pre_process(file):\n",
    "    source = file\n",
    "    df = pd.read_excel(source,encoding =\"utf-8\")\n",
    "    df=df.dropna()                                                 #å»æ‰ç©ºè¡Œ\n",
    "    scope=df['business_scope'].values.tolist()\n",
    "    industry=df['industry'].values.tolist()\n",
    "    \n",
    "    #åˆ†å¥\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i,j in zip(scope,industry):\n",
    "        #å»é™¤æ‰€æœ‰ç‰¹æ®Šç¬¦å·\n",
    "        sentence = []\n",
    "        string = re.sub('\\W+', '', i).replace(\"_\", '')\n",
    "        string = re.sub(r\"[0-9]\", \"\", string)\n",
    "        string = re.sub(r\"[a-zA-Z]\", \"\", string)\n",
    "        segs=jieba.lcut(str(string))                               #jiebaåˆ†è¯\n",
    "        for seg in segs:\n",
    "            if len(seg) == 1:\n",
    "                segs.remove(seg)\n",
    "            else:\n",
    "                sentence.append(seg)\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        #industryé›†åˆ\n",
    "        labels.append(j)\n",
    "        \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è½½å…¥è®­ç»ƒæ•°æ®é›†ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†\n",
    "sentences,labels = pre_process('./æ²ªäº¤æ‰€.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)  #è¯ç©ºé—´çš„ç”Ÿæˆï¼Œä¹Ÿå°±æ˜¯å°†æ‰€æœ‰æ–‡ç« ä¸­å–å‡ºæ¥å»é‡ï¼Œå‰©ä¸‹çš„è¯ç»„æˆçš„åˆ—è¡¨ã€‚å¹¶è¿›è¡Œç¼–å· \n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]     #é’ˆå¯¹æ¯ä¸ªæ–‡æœ¬ï¼Œå°†è¯æ±‡è½¬ä¸ºid\n",
    "#print(corpus[0])  # [(ID, frequence), (505, 1)...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è®­ç»ƒæ¨¡å‹\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100)\n",
    "#print(lda.print_topics(num_topics=100, num_words=5))\n",
    "\n",
    "# ä¿å­˜æ¨¡å‹\n",
    "lda.save('lda.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#åŠ è½½æ¨¡å‹\n",
    "lda = ldamodel.LdaModel.load('lda.model')\n",
    "\n",
    "#æµ‹è¯•æ•°æ®\n",
    "texts = pre_process('./ä¸Šäº¤æ‰€.xlsx')\n",
    "for text in texts:\n",
    "    bow = dictionary.doc2bow(text)\n",
    "    #print(lda.get_document_topics(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensiméƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é¢„å¤„ç†ï¼Œç”¨pandasè¯»å–csvæ–‡ä»¶ä¸­çš„ç‰¹å®šæ•°æ® _ç”¨äºgensimæ¨¡å‹\n",
    "def pre_process2(file):\n",
    "    source = file\n",
    "    df = pd.read_excel(source,encoding =\"utf-8\")\n",
    "    df=df.dropna()                                                 #å»æ‰ç©ºè¡Œ\n",
    "    scope=df['business_scope'].values.tolist()\n",
    "    industry=df['industry'].values.tolist()\n",
    "    \n",
    "    #åˆ†å¥\n",
    "    sentences=[]\n",
    "    for i,j in zip(scope,industry):\n",
    "        #å»é™¤æ‰€æœ‰ç‰¹æ®Šç¬¦å·\n",
    "        string = re.sub('\\W+', '', i).replace(\"_\", '')\n",
    "        segs=jieba.lcut(str(string))                               #jiebaåˆ†è¯\n",
    "        for seg in segs:\n",
    "            if len(seg) < 2:\n",
    "                segs.remove(seg)\n",
    "        sentences.append(\" \".join(segs))\n",
    "    \n",
    "    #å†™å…¥ä¸ºtxt\n",
    "    out=open('./train_data2.txt','w',encoding='utf-8')           \n",
    "    for sentence in sentences:\n",
    "        out.write(sentence+\"\\n\")\n",
    "    \n",
    "    return sentences    \n",
    "    \n",
    "    \n",
    "#è½½å…¥è®­ç»ƒæ•°æ®é›†ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†\n",
    "sentences2 = pre_process2('./æ²ªäº¤æ‰€.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#è®­ç»ƒæ¨¡å‹\n",
    "def train(model_name, file_name):\n",
    "    sent=word2vec.Text8Corpus(file_name)\n",
    "    model=word2vec.Word2Vec(sent, size=50)\n",
    "    model.save(model_name)\n",
    "\n",
    "    #å°†å­—å…¸ä¸­çš„è¯è¯­æ·»åŠ åˆ°æ•°ç»„word_in_vocä¸­\n",
    "    word_in_voc = []\n",
    "    for i, word in enumerate(model.wv.vocab):\n",
    "        word_in_voc.append(word)\n",
    "    #print(word_in_voc)\n",
    "    \n",
    "    return word_in_voc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_in_voc, model = train('word2vec_model', 'train_data2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ç­›é€‰å­—å…¸ä¸­çš„è¯\n",
    "def combination(sentences):\n",
    "    combine = []   #äº¤é›†\n",
    "    for sentence in sentences:\n",
    "        c = list(set(sentence).intersection(set(word_in_voc)))      #sentence[[a,b,c],[a,s,d]]ä¸å­—å…¸ä¸­è¯é€‰äº¤é›†\n",
    "        #print(c)\n",
    "        combine.append(c)\n",
    "\n",
    "    return combine\n",
    "    \n",
    "sentences = combination(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n"
     ]
    }
   ],
   "source": [
    "#å½¢æˆè¯å‘é‡åˆ†æ®µ50ç»´æ•°ç»„\n",
    "def wv50(sentences):\n",
    "    wv_arr = []\n",
    "    for sentence in sentences:\n",
    "        tmp = []\n",
    "        for i in range(len(sentence)):                            #[ [[wv],[wv]...] , [[wv],[wv]...] ]\n",
    "            tmp.append(model.wv[sentence[i]])                     #[[0,1,2,3],[2,3,4,5]..],[[0,1,2,3],[2,3,4,5]..]\n",
    "        wv_arr.append(tmp)\n",
    "    print(len(wv_arr))\n",
    "    return wv_arr\n",
    "\n",
    "wv_arr = wv50(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n"
     ]
    }
   ],
   "source": [
    "# åŸºäº2d PCAæ‹Ÿåˆæ•°æ®\n",
    "def minish_dimension(wv_arr):\n",
    "    result = []\n",
    "    for wv in wv_arr:\n",
    "        X = np.array(wv)                               #åºåˆ—è¯å‘é‡\n",
    "        pca = PCA(n_components=2)             #ä½¿ç”¨pcaå°†è¯å‘é‡é™ç»´åˆ°äºŒç»´\n",
    "        r = pca.fit_transform(X)                      #å°†åºåˆ—è¯å‘é‡ç»Ÿä¸€é™ä¸ºäºŒç»´\n",
    "        result.append(r)\n",
    "    \n",
    "    print(len(result))       #result :array( [ [wv1d],[wv1d]... ] , [[wv1d],[wv1d]...] , ...)     \n",
    "    return result\n",
    "\n",
    "result = minish_dimension(wv_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n"
     ]
    }
   ],
   "source": [
    "#æ®µè½å¹³å‡è¯å‘é‡ï¼Œå­˜å…¥wvListï¼Œå®ŒæˆKNNææ–™å‡†å¤‡\n",
    "def avg_wv(result):\n",
    "    wvList = []\n",
    "    for r in result:                       #[ [wv1d],[wv1d]... ]\n",
    "        sum0 = 0\n",
    "        sum1 = 0\n",
    "        for sub in r:                      # [wv1d]\n",
    "            sum0 += sub[0]\n",
    "            sum1 += sub[1]\n",
    "        wvList.append([ float(sum0/len(r)), float(sum1/len(r))])\n",
    "    print(len(wvList))\n",
    "    \n",
    "    return wvList\n",
    "\n",
    "wvList = avg_wv(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNNéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¸¤ä¸ªå‡†å¤‡æ•°ç»„ ï¼š wvList[] = [ [wv1d],[wv1d]... ] ï¼Œ labels[] = [asd,sf,fg,hj,...]\n",
    "\n",
    "def kNNClassify(testData, wvList, labels, k):\n",
    "    #æ±‚æ¯ç‚¹ä¹‹é—´çš„æœ€çŸ­è·ç¦»\n",
    "    #print(testData)\n",
    "    #print(len(wvList))\n",
    "    #print(len(labels))\n",
    "    #print(k)\n",
    "    diff = tile(testData, (len(wvList), 1)) - wvList       #æµ‹è¯•ç‚¹åˆ°æ¯ä¸ªæ•°æ®é›†çš„ç‚¹ä¸­[x,y]å·®å€¼\n",
    "    distance = sum(diff ** 2, axis = 1) ** 0.5             #distance = (diffx^2 + diffy^2)^0.5\n",
    "    \n",
    "    #æ±‚æœ€çŸ­è·ç¦»çš„æ•°æ®ç‚¹indexæ’åºé›†åˆï¼ˆå°-å¤§ï¼‰\n",
    "    arg = list(argsort(distance))                       #[ 337 1470 1421 ...   96  452  735] \n",
    "    freq_dict = {}                                          #é¢‘ç‡è¯å…¸\n",
    "    for i in range(k):\n",
    "        label = labels [ arg[0] ]                    #æœ€é‚»è¿‘kèŠ‚ç‚¹çš„åˆ†ç±»\n",
    "        freq_dict[label] = freq_dict.get(label, 0) + 1             #ç»Ÿè®¡æ¯ä¸ªè¯æ±‡å‡ºç°æ¬¡æ•°\n",
    "    #print(freq_dict)\n",
    "    #æ±‚å‡ºç°é¢‘ç‡æœ€é«˜çš„label\n",
    "    predict_label = max(freq_dict,key=freq_dict.get)\n",
    "    #print(predict_label)                                           #é¢‘ç‡æœ€é«˜çš„keyğŸŒŸ\n",
    "    max_freq = freq_dict [max(freq_dict,key=freq_dict.get)]\n",
    "    #print(max_freq)                                                #é¢‘ç‡æœ€é«˜çš„value\n",
    "    \n",
    "    return predict_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n",
      "2125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å…¨å›½åœ°äº§'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#æµ‹è¯•æ•°æ®\n",
    "kNNClassify([1,2], wvList, labels, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–°æ•°æ®\n",
    "def pre_process3(file):\n",
    "    source = file\n",
    "    df = pd.read_excel(source,encoding =\"utf-8\")\n",
    "    df=df.dropna()                                                 #å»æ‰ç©ºè¡Œ\n",
    "    scope=df['business_scope'].values.tolist()\n",
    "    industry=df['industry'].values.tolist()\n",
    "    \n",
    "    #åˆ†å¥\n",
    "    sentences=[]\n",
    "    for i,j in zip(scope,industry):\n",
    "        #å»é™¤æ‰€æœ‰ç‰¹æ®Šç¬¦å·\n",
    "        string = re.sub('\\W+', '', i).replace(\"_\", '')\n",
    "        segs=jieba.lcut(str(string))                               #jiebaåˆ†è¯\n",
    "        for seg in segs:\n",
    "            if len(seg) < 2:\n",
    "                segs.remove(seg)\n",
    "        sentences.append(\" \".join(segs))\n",
    "    \n",
    "    #å†™å…¥ä¸ºtxt\n",
    "    out=open('./train_data3.txt','w',encoding='utf-8')  \n",
    "    for sentence in sentences:\n",
    "        out.write(sentence+\"\\n\")\n",
    "    \n",
    "    return sentences    \n",
    "    \n",
    "#è½½å…¥è®­ç»ƒæ•°æ®é›†ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†\n",
    "sentences3 = pre_process3('./ä¸Šäº¤æ‰€.xlsx')                       #sentences3 : ['as as df','as r rg', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484\n",
      "1484\n"
     ]
    }
   ],
   "source": [
    "#å°†æµ‹è¯•é›†ä¸­çš„business scope ï¼ˆ1ï¼‰æ•°æ®æ¸…æ´— ï¼ˆ2ï¼‰åˆ†åˆ«è½¬ä¸ºäºŒç»´å‘é‡ ï¼ˆ3ï¼‰éªŒè¯å…¶kå–å€¼å‡†ç¡®ç‡ç»Ÿè®¡ï¼Œé€‰å‡ºæ­£ç¡®çš„k\n",
    "#ï¼ˆ1ï¼‰æ•°æ®æ¸…æ´—\n",
    "NEW_sentences,labels = pre_process('./ä¸Šäº¤æ‰€.xlsx')                 #NEW_sentences : [[as,as,as],[a,sg,fh],...]\n",
    "#ï¼ˆ2ï¼‰æ–°æ¨¡å‹+äºŒç»´å‘é‡\n",
    "NEW_word_in_voc, NEW_model = train('NEW_word2vec_model', 'train_data3.txt')\n",
    "NEW_sentences = combination(NEW_sentences)\n",
    "NEW_wv_arr = wv50(NEW_sentences)\n",
    "\n",
    "NEW_result = []\n",
    "L = []\n",
    "for wv in NEW_wv_arr:\n",
    "    for i in wv:\n",
    "        L.append(i.tolist())\n",
    "    X = np.array(L)                               #åºåˆ—è¯å‘é‡\n",
    "    pca = PCA(n_components=2)             #ä½¿ç”¨pcaå°†è¯å‘é‡é™ç»´åˆ°äºŒç»´\n",
    "    r = pca.fit_transform(X)                      #å°†åºåˆ—è¯å‘é‡ç»Ÿä¸€é™ä¸ºäºŒç»´\n",
    "    NEW_result.append(r)\n",
    "\n",
    "NEW_wvList = avg_wv(NEW_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./ä¸Šäº¤æ‰€.xlsx',encoding =\"utf-8\")\n",
    "df=df.dropna()                                                 #å»æ‰ç©ºè¡Œ\n",
    "industry=df['industry'].values.tolist()\n",
    "    \n",
    "NEW_labels = []\n",
    "for j in industry:\n",
    "    NEW_labels.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k value: 1 , with accuracy of: 2.7628032345013476 %\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•æ•°æ®é›†ä¸­çš„æ•°æ®\n",
    "i = 0\n",
    "accuracy = {}\n",
    "for vector in NEW_wvList:            #1484\n",
    "    for k in range(1,200):\n",
    "        predict_label = kNNClassify(vector, wvList, labels, k)            #æ”¾å›è®­ç»ƒé›†çš„listä¸­æ˜ å°„\n",
    "        if predict_label == NEW_labels[i]:\n",
    "            accuracy[k] = accuracy.get(k, 0) + 1\n",
    "    i += 1                                                     #æ­£ç¡®åˆ†ç±»    \n",
    "#æ±‚å‡ºç°é¢‘ç‡æœ€é«˜çš„label\n",
    "Best_k = max(accuracy,key=accuracy.get)\n",
    "print('Best k value:', Best_k, ', with accuracy of:', accuracy.get(k,0)/len(NEW_wvList)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNNClassify(vector, wvList, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•æ•°æ®é›†ä¸­çš„æ•°æ®\n",
    "i = 0\n",
    "accuracy = {}\n",
    "for vector in NEW_wvList:\n",
    "    predict_label = kNNClassify(vector, wvList, labels, 1)            #æ”¾å›è®­ç»ƒé›†çš„listä¸­æ˜ å°„\n",
    "    if predict_label == labels[i]:\n",
    "        accuracy[k] = accuracy.get(k, 0) + 1\n",
    "            \n",
    "i += 1                                                     #æ­£ç¡®åˆ†ç±»\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
