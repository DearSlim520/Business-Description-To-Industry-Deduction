{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import random\n",
    "import re\n",
    "from zhon.hanzi import punctuation\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import doc2vec, ldamodel\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预读取与预处理 _用于LDA模型\n",
    "def pre_process(file):\n",
    "    source = file\n",
    "    df = pd.read_excel(source,encoding =\"utf-8\")\n",
    "    df=df.dropna()                                                 #去掉空行\n",
    "    scope=df['business_scope'].values.tolist()\n",
    "    industry=df['industry'].values.tolist()\n",
    "    \n",
    "    #分句\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for i,j in zip(scope,industry):\n",
    "        #去除所有特殊符号\n",
    "        sentence = []\n",
    "        string = re.sub('\\W+', '', i).replace(\"_\", '')\n",
    "        string = re.sub(r\"[0-9]\", \"\", string)\n",
    "        string = re.sub(r\"[a-zA-Z]\", \"\", string)\n",
    "        segs=jieba.lcut(str(string))                               #jieba分词\n",
    "        for seg in segs:\n",
    "            if len(seg) == 1:\n",
    "                segs.remove(seg)\n",
    "            else:\n",
    "                sentence.append(seg)\n",
    "        sentences.append(sentence)\n",
    "        \n",
    "        #industry集合\n",
    "        labels.append(j)\n",
    "        \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#载入训练数据集，进行数据预处理\n",
    "sentences,labels = pre_process('./沪交所.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(sentences)  #词空间的生成，也就是将所有文章中取出来去重，剩下的词组成的列表。并进行编号 \n",
    "corpus = [dictionary.doc2bow(sentence) for sentence in sentences]     #针对每个文本，将词汇转为id\n",
    "#print(corpus[0])  # [(ID, frequence), (505, 1)...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "lda = ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=100)\n",
    "#print(lda.print_topics(num_topics=100, num_words=5))\n",
    "\n",
    "# 保存模型\n",
    "lda.save('lda.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "lda = ldamodel.LdaModel.load('lda.model')\n",
    "\n",
    "#测试数据\n",
    "texts = pre_process('./上交所.xlsx')\n",
    "for text in texts:\n",
    "    bow = dictionary.doc2bow(text)\n",
    "    #print(lda.get_document_topics(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理，用pandas读取csv文件中的特定数据 _用于gensim模型\n",
    "def pre_process2(file):\n",
    "    source = file\n",
    "    df = pd.read_excel(source,encoding =\"utf-8\")\n",
    "    df=df.dropna()                                                 #去掉空行\n",
    "    scope=df['business_scope'].values.tolist()\n",
    "    industry=df['industry'].values.tolist()\n",
    "    \n",
    "    #分句\n",
    "    sentences=[]\n",
    "    for i,j in zip(scope,industry):\n",
    "        #去除所有特殊符号\n",
    "        string = re.sub('\\W+', '', i).replace(\"_\", '')\n",
    "        segs=jieba.lcut(str(string))                               #jieba分词\n",
    "        for seg in segs:\n",
    "            if len(seg) < 2:\n",
    "                segs.remove(seg)\n",
    "        sentences.append(\" \".join(segs))\n",
    "    \n",
    "    #写入为txt\n",
    "    out=open('./train_data2.txt','w',encoding='utf-8')           \n",
    "    for sentence in sentences:\n",
    "        out.write(sentence+\"\\n\")\n",
    "    \n",
    "    return sentences    \n",
    "    \n",
    "    \n",
    "#载入训练数据集，进行数据预处理\n",
    "sentences2 = pre_process2('./沪交所.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#训练模型\n",
    "def train(model_name, file_name):\n",
    "    sent=word2vec.Text8Corpus(file_name)\n",
    "    model=word2vec.Word2Vec(sent, size=50)\n",
    "    model.save(model_name)\n",
    "\n",
    "    #将字典中的词语添加到数组word_in_voc中\n",
    "    word_in_voc = []\n",
    "    for i, word in enumerate(model.wv.vocab):\n",
    "        word_in_voc.append(word)\n",
    "    #print(word_in_voc)\n",
    "    \n",
    "    return word_in_voc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_in_voc, model = train('word2vec_model', 'train_data2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#筛选字典中的词\n",
    "def combination(sentences):\n",
    "    combine = []   #交集\n",
    "    for sentence in sentences:\n",
    "        c = list(set(sentence).intersection(set(word_in_voc)))      #sentence[[a,b,c],[a,s,d]]与字典中词选交集\n",
    "        #print(c)\n",
    "        combine.append(c)\n",
    "\n",
    "    return combine\n",
    "    \n",
    "sentences = combination(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n"
     ]
    }
   ],
   "source": [
    "#形成词向量分段50维数组\n",
    "def wv50(sentences):\n",
    "    wv_arr = []\n",
    "    for sentence in sentences:\n",
    "        tmp = []\n",
    "        for i in range(len(sentence)):                            #[ [[wv],[wv]...] , [[wv],[wv]...] ]\n",
    "            tmp.append(model.wv[sentence[i]])                     #[[0,1,2,3],[2,3,4,5]..],[[0,1,2,3],[2,3,4,5]..]\n",
    "        wv_arr.append(tmp)\n",
    "    print(len(wv_arr))\n",
    "    return wv_arr\n",
    "\n",
    "wv_arr = wv50(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n"
     ]
    }
   ],
   "source": [
    "# 基于2d PCA拟合数据\n",
    "def minish_dimension(wv_arr):\n",
    "    result = []\n",
    "    for wv in wv_arr:\n",
    "        X = np.array(wv)                               #序列词向量\n",
    "        pca = PCA(n_components=2)             #使用pca将词向量降维到二维\n",
    "        r = pca.fit_transform(X)                      #将序列词向量统一降为二维\n",
    "        result.append(r)\n",
    "    \n",
    "    print(len(result))       #result :array( [ [wv1d],[wv1d]... ] , [[wv1d],[wv1d]...] , ...)     \n",
    "    return result\n",
    "\n",
    "result = minish_dimension(wv_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n"
     ]
    }
   ],
   "source": [
    "#段落平均词向量，存入wvList，完成KNN材料准备\n",
    "def avg_wv(result):\n",
    "    wvList = []\n",
    "    for r in result:                       #[ [wv1d],[wv1d]... ]\n",
    "        sum0 = 0\n",
    "        sum1 = 0\n",
    "        for sub in r:                      # [wv1d]\n",
    "            sum0 += sub[0]\n",
    "            sum1 += sub[1]\n",
    "        wvList.append([ float(sum0/len(r)), float(sum1/len(r))])\n",
    "    print(len(wvList))\n",
    "    \n",
    "    return wvList\n",
    "\n",
    "wvList = avg_wv(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#两个准备数组 ： wvList[] = [ [wv1d],[wv1d]... ] ， labels[] = [asd,sf,fg,hj,...]\n",
    "\n",
    "def kNNClassify(testData, wvList, labels, k):\n",
    "    #求每点之间的最短距离\n",
    "    #print(testData)\n",
    "    #print(len(wvList))\n",
    "    #print(len(labels))\n",
    "    #print(k)\n",
    "    diff = tile(testData, (len(wvList), 1)) - wvList       #测试点到每个数据集的点中[x,y]差值\n",
    "    distance = sum(diff ** 2, axis = 1) ** 0.5             #distance = (diffx^2 + diffy^2)^0.5\n",
    "    \n",
    "    #求最短距离的数据点index排序集合（小-大）\n",
    "    arg = list(argsort(distance))                       #[ 337 1470 1421 ...   96  452  735] \n",
    "    freq_dict = {}                                          #频率词典\n",
    "    for i in range(k):\n",
    "        label = labels [ arg[0] ]                    #最邻近k节点的分类\n",
    "        freq_dict[label] = freq_dict.get(label, 0) + 1             #统计每个词汇出现次数\n",
    "    #print(freq_dict)\n",
    "    #求出现频率最高的label\n",
    "    predict_label = max(freq_dict,key=freq_dict.get)\n",
    "    #print(predict_label)                                           #频率最高的key🌟\n",
    "    max_freq = freq_dict [max(freq_dict,key=freq_dict.get)]\n",
    "    #print(max_freq)                                                #频率最高的value\n",
    "    \n",
    "    return predict_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125\n",
      "2125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'全国地产'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试数据\n",
    "kNNClassify([1,2], wvList, labels, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新数据\n",
    "def pre_process3(file):\n",
    "    source = file\n",
    "    df = pd.read_excel(source,encoding =\"utf-8\")\n",
    "    df=df.dropna()                                                 #去掉空行\n",
    "    scope=df['business_scope'].values.tolist()\n",
    "    industry=df['industry'].values.tolist()\n",
    "    \n",
    "    #分句\n",
    "    sentences=[]\n",
    "    for i,j in zip(scope,industry):\n",
    "        #去除所有特殊符号\n",
    "        string = re.sub('\\W+', '', i).replace(\"_\", '')\n",
    "        segs=jieba.lcut(str(string))                               #jieba分词\n",
    "        for seg in segs:\n",
    "            if len(seg) < 2:\n",
    "                segs.remove(seg)\n",
    "        sentences.append(\" \".join(segs))\n",
    "    \n",
    "    #写入为txt\n",
    "    out=open('./train_data3.txt','w',encoding='utf-8')  \n",
    "    for sentence in sentences:\n",
    "        out.write(sentence+\"\\n\")\n",
    "    \n",
    "    return sentences    \n",
    "    \n",
    "#载入训练数据集，进行数据预处理\n",
    "sentences3 = pre_process3('./上交所.xlsx')                       #sentences3 : ['as as df','as r rg', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484\n",
      "1484\n"
     ]
    }
   ],
   "source": [
    "#将测试集中的business scope （1）数据清洗 （2）分别转为二维向量 （3）验证其k取值准确率统计，选出正确的k\n",
    "#（1）数据清洗\n",
    "NEW_sentences,labels = pre_process('./上交所.xlsx')                 #NEW_sentences : [[as,as,as],[a,sg,fh],...]\n",
    "#（2）新模型+二维向量\n",
    "NEW_word_in_voc, NEW_model = train('NEW_word2vec_model', 'train_data3.txt')\n",
    "NEW_sentences = combination(NEW_sentences)\n",
    "NEW_wv_arr = wv50(NEW_sentences)\n",
    "\n",
    "NEW_result = []\n",
    "L = []\n",
    "for wv in NEW_wv_arr:\n",
    "    for i in wv:\n",
    "        L.append(i.tolist())\n",
    "    X = np.array(L)                               #序列词向量\n",
    "    pca = PCA(n_components=2)             #使用pca将词向量降维到二维\n",
    "    r = pca.fit_transform(X)                      #将序列词向量统一降为二维\n",
    "    NEW_result.append(r)\n",
    "\n",
    "NEW_wvList = avg_wv(NEW_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./上交所.xlsx',encoding =\"utf-8\")\n",
    "df=df.dropna()                                                 #去掉空行\n",
    "industry=df['industry'].values.tolist()\n",
    "    \n",
    "NEW_labels = []\n",
    "for j in industry:\n",
    "    NEW_labels.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k value: 1 , with accuracy of: 2.7628032345013476 %\n"
     ]
    }
   ],
   "source": [
    "#测试数据集中的数据\n",
    "i = 0\n",
    "accuracy = {}\n",
    "for vector in NEW_wvList:            #1484\n",
    "    for k in range(1,200):\n",
    "        predict_label = kNNClassify(vector, wvList, labels, k)            #放回训练集的list中映射\n",
    "        if predict_label == NEW_labels[i]:\n",
    "            accuracy[k] = accuracy.get(k, 0) + 1\n",
    "    i += 1                                                     #正确分类    \n",
    "#求出现频率最高的label\n",
    "Best_k = max(accuracy,key=accuracy.get)\n",
    "print('Best k value:', Best_k, ', with accuracy of:', accuracy.get(k,0)/len(NEW_wvList)*100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNNClassify(vector, wvList, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "#测试数据集中的数据\n",
    "i = 0\n",
    "accuracy = {}\n",
    "for vector in NEW_wvList:\n",
    "    predict_label = kNNClassify(vector, wvList, labels, 1)            #放回训练集的list中映射\n",
    "    if predict_label == labels[i]:\n",
    "        accuracy[k] = accuracy.get(k, 0) + 1\n",
    "            \n",
    "i += 1                                                     #正确分类\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
